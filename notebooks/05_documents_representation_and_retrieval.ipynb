{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T13:31:39.024124Z",
     "start_time": "2020-03-10T13:31:37.432083Z"
    }
   },
   "outputs": [],
   "source": [
    "#import logging\n",
    "#import codecs\n",
    "#import glob\n",
    "#import logging\n",
    "#import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More  document features\n",
    "\n",
    "\n",
    "We will denote by\n",
    "\n",
    "- $W= \\{w_1, \\dots, w_D\\}$ the set of words used to make the representations.\n",
    "- $X$ our corpus of documents.\n",
    "- $X_w$ the set of documents that contain word $w$. \n",
    "\n",
    "### Bag of words vector  (or `tf` vector)\n",
    "\n",
    "\n",
    "- The bag of words representation for a document $x$ given a vocabulary $W$, or the term frequency vector **$\\text{tf}(X;W)$** is defined as \n",
    "\n",
    "$$\n",
    "\\text{tf}(x;W) = \\left( \\#\\{w_1| w_1 \\in x\\}, \\dots, \\#\\{w_D| w_D \\in x\\})\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Term frequency Inverse Document frequency ( `tf * idf`)\n",
    "\n",
    "The objective of tf-idf representation is to emphasize the most relevant words of the documents. We want to emphasize:\n",
    "\n",
    "- Words that appear **frequently in the document**: term frequency \n",
    "- Words that appear **rarely in the corpus**: inverse document frequency\n",
    "\n",
    "#### Definition of the feature vectors\n",
    "\n",
    "\n",
    "- The **$\\text{tf}(X;W)$** vector for a document $x$ is defined as \n",
    "\n",
    "$$\n",
    "\\text{tf}(x;W) = \\left( \\#\\{w_1| w_1 \\in x\\}, \\dots, \\#\\{w_D| w_D \\in x\\})\\right)\n",
    "$$\n",
    "\n",
    "- The **$\\text{idf}(W; X)$** vector is defined as \n",
    "\n",
    "**$$\\text{idf}(W; X) = \\left( \\text{idf}(w_1; X), \\dots, \\text{idf}(w_D; X)\\right)$$** \n",
    "   \n",
    "$\\,\\,\\,\\,\\,\\,\\,$ A component of the feature for word $w \\in W$ in the corpus $X$ is defined as \n",
    "\n",
    "$$\n",
    "\\text{idf}(w, X) = log\\left(\\frac{|X|}{1+|X_{w}|}\\right)\n",
    "$$\n",
    "\n",
    "$\\,\\,\\,\\,\\,\\,\\,$Which simply means the full vector is \n",
    "$$\n",
    "\\text{idf}(w, X) = \\left( log\\left(\\frac{|X|}{1+|X_{w_1}|}\\right), \\dots, log\\left(\\frac{|X|}{1+|X_{w_D}|}\\right) \\right)\n",
    "$$\n",
    "\n",
    "- The tfidf vector for a document $x$ will be: $tf(x; X) * idf(X)$\n",
    "\n",
    "#### Observations\n",
    "\n",
    "- If a word appears in a few documents the idf vector will increase its weight.\n",
    "\n",
    "- If a word appears in a lots of documents documents the idf vector will decrease its weight.\n",
    "\n",
    "#### `sklearn.feature_extraction.text.TfidfVectorizer`\n",
    "\n",
    "- Notice that the implementation in sklearn already prevents zero divisions by default. This happens if `smooth_idf=True`.\n",
    "\n",
    "- By default the tfidf will only use words since `ngram_range=(1, 1)`. But this can be changed to allow n-grams in the feature vector components.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let us assume we have a corpus with one milion documents\n",
    "\n",
    "- Consider a word appearping in 100 documents:\n",
    "\n",
    "$$\\log\\left(\\frac{1000.000}{1 + 100} \\right) = 9.200$$\n",
    "\n",
    "- Consider a word appearing in 100.000 documents\n",
    "\n",
    "$$\\log\\left(\\frac{1000.000}{1 + 100.000} \\right) = 2.197$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T13:31:39.030148Z",
     "start_time": "2020-03-10T13:31:39.025624Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-554637a8d8ff>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-554637a8d8ff>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    if word_inds:\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def build_vocabulary(corpus, splitter):\n",
    "    \"\"\"\n",
    "    This function has to return X_w, a dict containing for each key, how\n",
    "    many documents having that key are in our corpus.\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    X_w = dict()\n",
    "    \n",
    "    for document in corpus:\n",
    "        words      = set(splitter.findall(document.lower()))\n",
    "        # fill up vocabulary \n",
    "        \n",
    "        # fill up X_w\n",
    "        \n",
    "    return vocabulary, X_w\n",
    "\n",
    "def term_frequency(document, word_to_ind, splitter, \n",
    "                   normalize=True, word_inds=False):\n",
    "    \n",
    "    words = splitter.findall(document.lower())\n",
    "    n_features = len(word_to_ind)\n",
    "    tf = sp.sparse.lil_matrix( (1, n_features), dtype=float)\n",
    "    \n",
    "    word_indices = []\n",
    "    for w in words:\n",
    "        ## fill up word_indices\n",
    "        \n",
    "        ## fill up tf\n",
    "        \n",
    "    if word_inds:\n",
    "        if normalize:\n",
    "            return tf.multiply(1/sp.sparse.linalg.norm(tf))\n",
    "        else:\n",
    "            return tf\n",
    "    else:\n",
    "        if normalize:\n",
    "            return tf.multiply(1/sp.sparse.linalg.norm(tf))\n",
    "        else:\n",
    "            return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.6 s, sys: 10.7 s, total: 1min 9s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "splitter = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "%time vocabulary, X_w = build_vocabulary(newsgroups_train.data, splitter)\n",
    "\n",
    "word_to_ind = {v:i for i,v in enumerate(vocabulary)}\n",
    "ind_to_word = {v:k for k,v in word_to_ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.47 ms, sys: 803 Âµs, total: 4.27 ms\n",
      "Wall time: 3.95 ms\n"
     ]
    }
   ],
   "source": [
    "%time tf = term_frequency(newsgroups_train.data[0],\\\n",
    "                          word_to_ind, splitter, word_inds=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the term frequency is OK, compare with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.77 s, sys: 107 ms, total: 3.88 s\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "tfidf_sk = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=False,\n",
    "                                                           smooth_idf=False, \n",
    "                                                           sublinear_tf=False)\n",
    "\n",
    "%time tfidf_sk.fit(newsgroups_train.data)\n",
    "\n",
    "inverse_vocabulary_ = {v: k for k, v in tfidf_sk.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 ms, sys: 1.55 ms, total: 3.75 ms\n",
      "Wall time: 10.1 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/envs/py3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "%time x_sk = tfidf_sk.transform([newsgroups_train.data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(tf.sum(), x_sk.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_x_own = [ind_to_word[k] for k in tf.nonzero()[1]]\n",
    "words_x_sk = [inverse_vocabulary_[k] for k in x_sk.nonzero()[1]]\n",
    "set(words_x_own) == set(words_x_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Generate tfidf and compare with sklearn \n",
    "\n",
    "Finish the `compute_idf` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_idf(X_w, word_to_ind, n_documents):\n",
    "\n",
    "    n_features = len(word_to_ind)\n",
    "    #idf = sp.sparse.csr_matrix( (1, n_features), dtype=float)\n",
    "    idf = np.zeros([1, n_features])\n",
    "    \n",
    "    for w in X_w:\n",
    "        # fill up idf\n",
    "        pass\n",
    "    \n",
    "    #idf = idf + 1    \n",
    "    return sp.sparse.csr_matrix(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 299 ms, sys: 5.5 ms, total: 305 ms\n",
      "Wall time: 305 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# lil_matrix is more efficient.\n",
    "tf = term_frequency(newsgroups_train.data[0], word_to_ind,\\\n",
    "                    splitter, normalize=False,word_inds=False)\n",
    "\n",
    "idf = compute_idf(X_w,word_to_ind, len(newsgroups_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.640737377507692, 1.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf.max(), idf.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_documents = len(X_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = tf.multiply(idf)\n",
    "tfidf = tfidf/sp.sparse.linalg.norm(tfidf)\n",
    "sp.sparse.linalg.norm(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(newsgroups_train.data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/envs/py3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf_sklearn = tfidf_vectorizer.transform(newsgroups_train.data[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.data.dtype, tfidf_sklearn.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.69781523302251 7.697815233022508\n",
      "\n",
      "sklearn tfidf and our tfidf are the same: True\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.sum(), tfidf_sklearn.sum())\n",
    "print(\"\\nsklearn tfidf and our tfidf are the same:\",\n",
    "      np.isclose(tfidf_sklearn.sum(),tfidf.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf in data \n",
    "\n",
    "Now we will use a dataframe containing text and use the tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people = pd.read_csv('people_wiki_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                URI  \\\n",
       "0           0        <http://dbpedia.org/resource/Digby_Morrell>   \n",
       "1           1       <http://dbpedia.org/resource/Alfred_J._Lewy>   \n",
       "2           2        <http://dbpedia.org/resource/Harpdog_Brown>   \n",
       "3           3  <http://dbpedia.org/resource/Franz_Rottensteiner>   \n",
       "4           4               <http://dbpedia.org/resource/G-Enka>   \n",
       "\n",
       "                  name                                               text  \n",
       "0        Digby Morrell  digby morrell born 10 october 1979 is a former...  \n",
       "1       Alfred J. Lewy  alfred j lewy aka sandy lewy graduated from un...  \n",
       "2        Harpdog Brown  harpdog brown is a singer and harmonica player...  \n",
       "3  Franz Rottensteiner  franz rottensteiner born in waidmannsfeld lowe...  \n",
       "4               G-Enka  henry krvits born 30 december 1974 in tallinn ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     int64\n",
       "URI           object\n",
       "name          object\n",
       "text          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'franz rottensteiner born in waidmannsfeld lower austria austria on 18 january 1942 is an austrian publisher and critic in the fields of science fiction and the fantasticrottensteiner studied journalism english and history at the university of vienna receiving his doctorate in 1969 he served about fifteen years as librarian and editor at the sterreichisches institut fr bauforschung in vienna in addition he produced a number of translations into german of leading sf authors including herbert w franke stanislaw lem philip k dick kobo abe cordwainer smith brian w aldiss and the strugatski brothersin 1973 his new york anthology view from another shore of european science fiction introduced a number of continental authors to the englishreading public some of the authors in the work are stanislaw lem josef nesvadba gerard klein and jeanpierre andrevonthe year 1975 saw the start of his series die phantastischen romane for seven years it republished works of both lesser and betterknown writers as well as new ones ending with a total of 28 volumes in the years 19791985 he brought out trnaslations of h g wellss works in an eighteen volumes seriesrottensteiner provoked some controversy with his negative assessment of american science fiction what matters is the highest achievements and there the us has yet to produce a figure comparable to hg wells olaf stapledon karel apek or stanisaw lem rottensteiner describedroger zelazny barry n malzberg and robert silverberg as producing travesties of fiction and stated asimov is a typical nonwriter and heinlein and andersonare just banal however rottensteiner praised philip k dick listing him as one of the greatest sf writers from 1980 through 1998 he was advisor for suhrkamp verlags phantastische bibliothek which brought out some three hundred books in all he has edited about fifty anthologies produced two illustrated books the science fiction book 1975 und the fantasy book 1978 as well as working on numerous reference works on science fiction his close association with and promotion of lem until 1995 was a factor in the recognition of the latter in the united statesrottensteiner has been the editor of quarber merkur the leading german language critical journal of science fiction since 1963 in 2004 on the occasion of the hundredth number of this journal he was awarded a special kurdlawitzpreis'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people[\"text\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: get_text_given_name\n",
    "\n",
    "build a function  `get_text_given_name` that returns the text of a particular person from the data.\n",
    "\n",
    "First try to think how can you select the text for Barack Obama..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obama = people[people['name'] == 'Barack Obama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35817    barack hussein obama ii brk husen bm born augu...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people[people['name'] == 'Barack Obama'][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(df, boolean_series):\n",
    "    row_df = df[boolean_series].text\n",
    "    return df.loc[row_df.index[0]].text\n",
    "\n",
    "def get_text_given_name(df, name):\n",
    "    # TODO\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what you would expect for Obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'barack hussein obama ii brk husen bm born august 4 1961 is the 44th and current president of the united states and the first african american to hold the office born in honolulu hawaii obama is a graduate of columbia university and harvard law school where he served as president of the harvard law review he was a community organizer in chicago before earning his law degree he worked as a civil rights attorney and taught constitutional law at the university of chicago law school from 1992 to 2004 he served three terms representing the 13th district in the illinois senate from 1997 to 2004 running unsuccessfully for the united states house of representatives in 2000in 2004 obama received national attention during his campaign to represent illinois in the united states senate with his victory in the march democratic party primary his keynote address at the democratic national convention in july and his election to the senate in november he began his presidential campaign in 2007 and after a close primary campaign against hillary rodham clinton in 2008 he won sufficient delegates in the democratic party primaries to receive the presidential nomination he then defeated republican nominee john mccain in the general election and was inaugurated as president on january 20 2009 nine months after his election obama was named the 2009 nobel peace prize laureateduring his first two years in office obama signed into law economic stimulus legislation in response to the great recession in the form of the american recovery and reinvestment act of 2009 and the tax relief unemployment insurance reauthorization and job creation act of 2010 other major domestic initiatives in his first term included the patient protection and affordable care act often referred to as obamacare the doddfrank wall street reform and consumer protection act and the dont ask dont tell repeal act of 2010 in foreign policy obama ended us military involvement in the iraq war increased us troop levels in afghanistan signed the new start arms control treaty with russia ordered us military involvement in libya and ordered the military operation that resulted in the death of osama bin laden in january 2011 the republicans regained control of the house of representatives as the democratic party lost a total of 63 seats and after a lengthy debate over federal spending and whether or not to raise the nations debt limit obama signed the budget control act of 2011 and the american taxpayer relief act of 2012obama was reelected president in november 2012 defeating republican nominee mitt romney and was sworn in for a second term on january 20 2013 during his second term obama has promoted domestic policies related to gun control in response to the sandy hook elementary school shooting and has called for full equality for lgbt americans while his administration has filed briefs which urged the supreme court to strike down the defense of marriage act of 1996 and californias proposition 8 as unconstitutional in foreign policy obama ordered us military involvement in iraq in response to gains made by the islamic state in iraq after the 2011 withdrawal from iraq continued the process of ending us combat operations in afghanistan and has sought to normalize us relations with cuba'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_given_name(people, \"Barack Obama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the tfidf to get a representation of the vector containing the description of obama and Emma Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/envs/py3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "obama_vec, emma_vec = tfidf.fit_transform([get_text_given_name(people, \"Barack Obama\"), \n",
    "                                           get_text_given_name(people, \"Emma Watson\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/envs/py3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 s, sys: 567 ms, total: 16.9 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_tfidf = tfidf.fit_transform(people[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scp\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Build the function `get_closest_k_names(tfidf_vec, X_tfidf, k=10)` that returns the names of the people associated to the text that is closer to the query text.\n",
    "\n",
    "To do so, use the cosine_similarity\n",
    "\n",
    "Try to find the closest names to:\n",
    "\n",
    "```\n",
    "\"Brad Pitt\"\n",
    "\"Angelina Jolie\"\n",
    "\"Barack Obama\"\n",
    "\"Bill Clinton\"\n",
    "\"Emma Watson\"\n",
    "```\n",
    "\n",
    "Do they make any sense? (You might want to check the wikipedia)\n",
    "\n",
    "You will need to compute the tfidf for each individual in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/envs/py3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "brad_pitt_tfidf = # fill in\n",
    "angelina_tfidf  = # fill in\n",
    "obama_tfidf     = # fill in\n",
    "bill_tfidf      = # fill in\n",
    "emma_tfidf      = # fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_closest_k_names(tfidf_vec, X_tfidf, k=10):\n",
    "    # fill in \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know if it is working you should get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29609         Bettina Devin\n",
       "8504     Margaret C. Snyder\n",
       "3633        Priyanka Chopra\n",
       "26909      Pat Studdy-Clift\n",
       "11666            Jane Fonda\n",
       "34756          Maggie Smith\n",
       "35902     Natashia Williams\n",
       "8973           John Granger\n",
       "17821         Emma Thompson\n",
       "3115           Stuart Craig\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_k_names(emma_tfidf, X_tfidf, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19416         Donnie Fowler\n",
       "11723           Howard Dean\n",
       "37166    Richard L. Barclay\n",
       "9517          Deval Patrick\n",
       "28453            Jill Alper\n",
       "35817          Barack Obama\n",
       "2092     Richard Blumenthal\n",
       "28447        George W. Bush\n",
       "4096       Sheffield Nelson\n",
       "25658           Dick Morris\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_k_names(bill_tfidf, X_tfidf, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24263      Jessica Lange\n",
       "11666         Jane Fonda\n",
       "11156      Anne Hathaway\n",
       "28076          Amy Adams\n",
       "33529     Cate Blanchett\n",
       "24426          Brad Pitt\n",
       "21644       Jodie Foster\n",
       "16242       Meryl Streep\n",
       "34756       Maggie Smith\n",
       "29009    Barbara Hershey\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_k_names(angelina_tfidf, X_tfidf, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Finding similar documents\n",
    "\n",
    "\n",
    "Build a function `closest_point` that given a set of vectors `all_points`,a query vector `query_point` and a distance measure between vectors `dist` finds the vector from `all_points` that is the closest to `query_point`  (according to `dist`). Moreover, return also the distace between this closest point and and the query point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def closest_point(all_points, query_point, dist):\n",
    "    # FILL IN THIS\n",
    "            \n",
    "    return closest_point_, closest_distance_\n",
    "\n",
    "def dist(x,y):\n",
    "    return np.sqrt(np.linalg.norm((x-y)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if your code is correct make the following plot. Is the blue point the closest point to the query point (red point) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: [1 3]\n",
      "Closest to query: [0 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a26679320>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a13b6b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X = np.array([[1,1],[0,3],[4,3],[0,0],[1,5],[6,1],[-2,1],[4,4],[2,1],[-1,0]])\n",
    "x_components = [x[0] for x in X]\n",
    "y_components = [x[1] for x in X]\n",
    "query = np.array([1,3])\n",
    "\n",
    "closest_to_query, _ = closest_point(X, query, dist)\n",
    "print(\"Query: {}\\nClosest to query: {}\".format(query,closest_to_query))\n",
    "plt.scatter(x_components, y_components, color=\"black\")\n",
    "plt.scatter(query[0], query[1], color=\"red\")\n",
    "plt.scatter(closest_to_query[0], closest_to_query[1], color=\"blue\", linewidths=1, marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Finding closest point in numpy\n",
    "\n",
    "There is a efficient way to find the closest point to a query point using numpy.\n",
    "Use the functions `np.argmin` and `np.argsort` in order to find the closest point to a query point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import neighbors\n",
    "\n",
    "n_features = 20\n",
    "X = np.random.rand(10000000,n_features).astype(np.float32)\n",
    "x = np.random.rand(1,n_features).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.03 s, sys: 405 ms, total: 1.44 s\n",
      "Wall time: 1.21 s\n",
      "CPU times: user 987 ms, sys: 360 ms, total: 1.35 s\n",
      "Wall time: 1.1 s\n",
      "\n",
      "closest row from x is 5245803\n"
     ]
    }
   ],
   "source": [
    "%time distances =  np.mean((X-x)**2,1)\n",
    "#One second is too much\n",
    "%time closest = np.argmin(np.mean((X-x)**2,1))\n",
    "print(\"\\nclosest row from x is {}\".format(closest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013441379"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5245803, 3507758, 9996929, ..., 1606706,  114629, 7864903])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "closest = np.argmin(np.mean((X-x)**2,1))\n",
    "e = time.time()\n",
    "numpy_time = abs(s-e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom function for closest point\n",
    "\n",
    "The implementation will be slower (but this is due to python runetime!, if you would have coded it  it in C or cython it would be faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 5s, sys: 1.29 s, total: 2min 6s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "closest_point_, closest_distance_ = closest_point(X, x, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Kd tree\n",
    "\n",
    "In the event that you want to build a system that needs to find similar points to a database of points, using a data structure for fast search is crucial. Now we will learn a little bit about a KD-Tree.\n",
    "\n",
    "KD-trees are an efficient structure for efficiently representing our data. KD-trees provide an organization of our documents in terms of a certain partitioning of our space. The organization is based on recursively partitioning points into axis, defining \"boxes\".\n",
    "\n",
    "The KD-tree structure is based on making aligned cuts and maintaining lists of points that fall into each one of these different bins. This structure allows us  efficiently prune our search space so that we do not have to visit every single data point, for every query, necessarily. Sometimes we will have to do it but hopefully, in many cases, we will not have to do it.\n",
    "\n",
    "\n",
    "#### Using KD-trees\n",
    "\n",
    "Let us see how KD-trees can aid in efficiently making NN search. Let us assume we are given a KD_tree and let us see how to ue it. Later on we will see how to build the tree.\n",
    "\n",
    "Given a query point $\\bf{x}$:\n",
    "\n",
    "- Traverse the tree until the query point is reached. That is, check all the conditions of the KD-tree for the query point until a leave is reached.\n",
    "    - Once the query point is found save the \"box\" where it is found.\n",
    "    \n",
    "    \n",
    "- Compute the distance between each neighbor in the box and the query point.\n",
    "\n",
    "\n",
    "- Record the smallest distance to the NN so far.\n",
    "\n",
    "\n",
    "- Backtrack and try other branch at each node visited.\n",
    "    - Use the distance bound and bounding box of each node to prune parts of the three that cannot include the nearest neighbor.\n",
    "         \n",
    "         That is, **if the smallest distance is less than the distance from the query point to the bounding box there is no need to compute the distance between any point in the bounding box to the query point**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build a KD-Tree using scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tree = sklearn.neighbors.KDTree(X, leaf_size=1_000)\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree = sklearn.neighbors.KDTree(X, leaf_size=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "s = time.time()\n",
    "distance_to_closest, closest_kdtree = tree.query(x, k=1)\n",
    "e = time.time()\n",
    "kdtree_time = abs(s-e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nclosest row from x is {}\".format(closest_kdtree[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you noteice any difference? The bigger the dataset is the bigger the difference will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy time: 0.6174180507659912\n",
      "Kdtree time: 0.2856941223144531\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy time:\", numpy_time)\n",
    "print(\"Kdtree time:\", kdtree_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building a kdtree with less leaves\n",
    "\n",
    "Notice that in scikit-learn KD-trees have a parameter: \n",
    "\n",
    "leaf_size : positive integer (default = 40)\n",
    "\n",
    "This is the number of points at which to switch to brute-force search. Changing leaf_size will not affect the results of a query, but can significantly impact the speed of a query and the memory required to store the constructed tree.\n",
    "\n",
    "The amount of memory needed to store the tree scales as approximately `n_samples / leaf_size`. For a specified leaf_size, a leaf node is guaranteed to satisfy `leaf_size <= n_points <= 2 * leaf_size`, except in the case that `n_samples < leaf_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Tunning KD-tree objects\n",
    "\n",
    "- build different KDTrees with different leaf_size values (for example 10, and 100 and 1000). Do you see any speedup when making searches of a nearest neighbour?\n",
    "\n",
    "- Inspect the parameter k. Try `tree.query(x,k=10)`. What do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy time: 0.6174180507659912\n",
      "Kdtree(1000) time: 0.2856941223144531\n",
      "Kdtree(10)   time: 0.08020997047424316\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy time:\", numpy_time)\n",
    "print(\"Kdtree(1000) time:\", kdtree_time)\n",
    "print(\"Kdtree(10)   time:\", kdtree_time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use also the kd tree to get the k closest items to our query vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 176 ms, sys: 1.18 ms, total: 177 ms\n",
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%time distances_to_closest, close_kdtree = tree.query(x,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
