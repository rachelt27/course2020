{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing-words\" data-toc-modified-id=\"Preprocessing-words-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocessing words</a></span><ul class=\"toc-item\"><li><span><a href=\"#Steming\" data-toc-modified-id=\"Steming-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Steming</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Lemmatization</a></span></li></ul></li><li><span><a href=\"#Features-for-documents\" data-toc-modified-id=\"Features-for-documents-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Features for documents</a></span><ul class=\"toc-item\"><li><span><a href=\"#From-docs-to-feature-vectors:-Make-your-own-countvectorizer\" data-toc-modified-id=\"From-docs-to-feature-vectors:-Make-your-own-countvectorizer-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>From docs to feature vectors: Make your own countvectorizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Customising-Vectoriser-classes\" data-toc-modified-id=\"Customising-Vectoriser-classes-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Customising Vectoriser classes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-of-how-to-encode-sparse-matrix-fast\" data-toc-modified-id=\"Example-of-how-to-encode-sparse-matrix-fast-2.1.1.1\"><span class=\"toc-item-num\">2.1.1.1&nbsp;&nbsp;</span>Example of how to encode sparse matrix fast</a></span></li></ul></li><li><span><a href=\"#Exercise:-Build-a-Simple-countvectorizer\" data-toc-modified-id=\"Exercise:-Build-a-Simple-countvectorizer-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Exercise: Build a Simple countvectorizer</a></span></li></ul></li><li><span><a href=\"#Training-a-document-classifier-with-SimpleCountVectorizer\" data-toc-modified-id=\"Training-a-document-classifier-with-SimpleCountVectorizer-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Training a document classifier with <code>SimpleCountVectorizer</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#I)-No-Stemmer-and-no-doc_cleaner\" data-toc-modified-id=\"I)-No-Stemmer-and-no-doc_cleaner-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>I) No Stemmer and no doc_cleaner</a></span></li><li><span><a href=\"#II)-No-stemmer-but-doc_cleaner\" data-toc-modified-id=\"II)-No-stemmer-but-doc_cleaner-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>II) No stemmer but doc_cleaner</a></span></li><li><span><a href=\"#III)-Use-a-SnowballStemmer\" data-toc-modified-id=\"III)-Use-a-SnowballStemmer-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>III) Use a SnowballStemmer</a></span></li><li><span><a href=\"#Table-with-results-for-each-pipeline\" data-toc-modified-id=\"Table-with-results-for-each-pipeline-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Table with results for each pipeline</a></span></li></ul></li><li><span><a href=\"#Ngram-features-with-Sklearn-vectorizer\" data-toc-modified-id=\"Ngram-features-with-Sklearn-vectorizer-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Ngram features with Sklearn vectorizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#IV)-Training-a-document-classifier-with-sklearn-CountVectorizer\" data-toc-modified-id=\"IV)-Training-a-document-classifier-with-sklearn-CountVectorizer-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>IV) Training a document classifier with sklearn <code>CountVectorizer</code></a></span></li><li><span><a href=\"#V)-Training-a-document-classifier-with-sklearn-CountVectorizer-and-ngrams\" data-toc-modified-id=\"V)-Training-a-document-classifier-with-sklearn-CountVectorizer-and-ngrams-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>V) Training a document classifier with sklearn <code>CountVectorizer</code> and ngrams</a></span></li></ul></li></ul></li><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Feature selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#SelectKbest\" data-toc-modified-id=\"SelectKbest-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>SelectKbest</a></span></li><li><span><a href=\"#2.3.3)-Feature-Union\" data-toc-modified-id=\"2.3.3)-Feature-Union-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>2.3.3) Feature Union</a></span></li></ul></li><li><span><a href=\"#2.4)-Crossvalidating-results-(Exercise)\" data-toc-modified-id=\"2.4)-Crossvalidating-results-(Exercise)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>2.4) Crossvalidating results (Exercise)</a></span></li><li><span><a href=\"#2.5-Hashing-words-(Exercise)\" data-toc-modified-id=\"2.5-Hashing-words-(Exercise)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>2.5 Hashing words (Exercise)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "SUMMARY OF TODAY \n",
    "\n",
    "# [2] NLP features from language\n",
    "\n",
    "##  Preprocessing words\n",
    "\n",
    "- Steaming \n",
    "- Lematization \n",
    "  \n",
    "\n",
    "## 2) Features for documents\n",
    "\n",
    "We can encode documents in lots of ways. Let us explore several strategies for converting raw strings to feature vectors representing whole documents.\n",
    "\n",
    "### 2.1) From docs to feature vectors: Make your own countvectorizer\n",
    "   - I)   No cleaning, no lemmatization\n",
    "   - II)  Yes cleaning, no lemmatization\n",
    "   - III) Yes cleaning, yes lemmatization\n",
    "\n",
    "### 2.2)  Ngram features with Sklearn vectorizer\n",
    "Use the standard vectorizer from sklaern\n",
    "\n",
    "   \n",
    "\n",
    "### 2.3) Feature selection\n",
    "\n",
    "We can have too many unreliable words (and/for combinations of words). Prunning some of this features might help us to generalize better. \n",
    " \n",
    "### 2.4) Crossvalidation (Exercise)\n",
    "\n",
    "We Crosvalidation with a pipeline to explore the different choices you make in the whole program. This can take some time. Use Random Crossvalidation to more efficiently explore the space if your hardware requirements are limited for the problem at hand.\n",
    "\n",
    "\n",
    "   \n",
    "### 2.5) Investigate hashing (Exercise)\n",
    "\n",
    "Use `feature_extraction.FeatureHasher` to generate your vectors. What performance do you get?\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing words\n",
    "\n",
    "### Steming\n",
    "\n",
    "Stemming consist on removing the suffixes or prefixes used in word. The returned string from a lemmatizer might not be a valid word from the language.\n",
    "\n",
    "`Stem(saw) = saw`\n",
    "\n",
    "- Potter algorithm (1980), Lovins stemmer, Husk Stemmer (1990) are relevant algorithms to do steamming.\n",
    "\n",
    "\n",
    "We can use steamming adn lemmatization to reduce to a similar meaning different forms of similar words\n",
    "\n",
    "- Example:\n",
    "   \n",
    "    - `are`, `is` => `be`\n",
    "    - `man`, `man`, `man'`\n",
    "    \n",
    "Using this process we can trasnform `Dogs are Man's best friend` to `Dog be Man best friend`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter    = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                PorterStemmer       LancasterStemmer    \n",
      "\n",
      "dogs                dog                 dog                 \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n",
      "pass                pass                pass                \n",
      "passing             pass                pass                \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "passed              pass                pass                \n",
      "trouble             troubl              troubl              \n",
      "troubling           troubl              troubl              \n",
      "care                care                car                 \n",
      "believes            believ              believ              \n"
     ]
    }
   ],
   "source": [
    "words = [\"dogs\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\n",
    "         \"football\",\"pass\",\"passing\",\"friendship\", \"friends\", \"friendships\",\n",
    "         \"passed\",\"trouble\",\"troubling\",\"care\", \"believes\"]\n",
    "preprocess = [porter, lancaster]\n",
    "\n",
    "len_bin = 20\n",
    "col_formater = \"{0:len_bin}{1:len_bin}{2:len_bin}\".replace(\"len_bin\",str(len_bin))\n",
    "print(col_formater.format(\"Word\", porter.__class__.__name__, lancaster.__class__.__name__))\n",
    "print(\"\")\n",
    "for w in words:\n",
    "    print( col_formater.format(w, porter.stem(w), lancaster.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def stem(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    for word in token_words:\n",
    "        sentence_stemmed.append(porter.stem(word))\n",
    "        sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j.k. rowl wrote harri potter . she never expect the book to be famou . '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"J.K. Rowling wrote Harry Potter. She never expected the book to be famous.\"\n",
    "stem(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J.K. Rowling wrote Harry Potter.',\n",
       " 'She never expected the book to be famous.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J',\n",
       " 'K',\n",
       " ' Rowling wrote Harry Potter',\n",
       " ' She never expected the book to be famous',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Be carefull separating phrases\n",
    "s.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J.K.',\n",
       " 'Rowling',\n",
       " 'wrote',\n",
       " 'Harry',\n",
       " 'Potter',\n",
       " '.',\n",
       " 'She',\n",
       " 'never',\n",
       " 'expected',\n",
       " 'the',\n",
       " 'book',\n",
       " 'to',\n",
       " 'be',\n",
       " 'famous',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lemmatization\n",
    "\n",
    "\n",
    "Lemmatization consist on properly use of a vocabulary and morphological analysis of words, aiming to remove inflectional endings only with the goal of returning any word to a set of base (or dictionary form) words.\n",
    "\n",
    "\n",
    "`Lemmatize(saw) = see`\n",
    "\n",
    "\n",
    "We will use a lemmatizer from WordNet (https://wordnet.princeton.edu) avaliable from nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I was running and eating. This was a terrible idea.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "I                   I                   \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "This                This                \n",
      "was                 wa                  \n",
      "a                   a                   \n",
      "terrible            terrible            \n",
      "idea                idea                \n"
     ]
    }
   ],
   "source": [
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the words did no change!\n",
    "\n",
    "This is because there was no context. If we give a part of speech type then the lemmatizer will do what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "I                   I                   \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "This                This                \n",
      "was                 be                  \n",
      "a                   a                   \n",
      "terrible            terrible            \n",
      "idea                idea                \n"
     ]
    }
   ],
   "source": [
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                PorterStemmer       LancasterStemmer    WordNetLemmatizer   \n",
      "\n",
      "dogs                dog                 dog                 dog                 \n",
      "destabilize         destabil            dest                destabilize         \n",
      "misunderstanding    misunderstand       misunderstand       misunderstanding    \n",
      "railroad            railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             football            \n",
      "pass                pass                pass                pas                 \n",
      "passing             pass                pass                passing             \n",
      "friendship          friendship          friend              friendship          \n",
      "friends             friend              friend              friend              \n",
      "friendships         friendship          friend              friendship          \n",
      "passed              pass                pass                passed              \n",
      "trouble             troubl              troubl              trouble             \n",
      "troubling           troubl              troubl              troubling           \n",
      "care                care                car                 care                \n",
      "believes            believ              believ              belief              \n"
     ]
    }
   ],
   "source": [
    "words = [\"dogs\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\n",
    "         \"football\",\"pass\",\"passing\",\"friendship\", \"friends\", \"friendships\",\n",
    "         \"passed\",\"trouble\",\"troubling\",\"care\", \"believes\"]\n",
    "preprocess = [porter, lancaster, wordnet_lemmatizer]\n",
    "\n",
    "len_bin = 20\n",
    "col_formater = \"{0:len_bin}{1:len_bin}{2:len_bin}{3:len_bin}\".replace(\"len_bin\",str(len_bin))\n",
    "print(col_formater.format(\"Word\", porter.__class__.__name__, lancaster.__class__.__name__, wordnet_lemmatizer.__class__.__name__))\n",
    "print(\"\")\n",
    "for w in words:\n",
    "    print( col_formater.format(w, porter.stem(w), lancaster.stem(w), wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Features for documents\n",
    "\n",
    "\n",
    "###  From docs to feature vectors: Make your own countvectorizer\n",
    "\n",
    "\n",
    "Let us build a simple document classifier featurizing each document by word counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.datasets\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sklearn.datasets.fetch_20newsgroups()\n",
    "\n",
    "X_train = sklearn.datasets.fetch_20newsgroups(subset=\"train\").data\n",
    "y_train = sklearn.datasets.fetch_20newsgroups(subset=\"train\").target\n",
    "X_test  = sklearn.datasets.fetch_20newsgroups(subset=\"test\").data\n",
    "y_test  = sklearn.datasets.fetch_20newsgroups(subset=\"test\").target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X[\"data\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customising Vectoriser classes\n",
    "\n",
    "- **preprocessor**: a callable that takes an entire document as input (as a single string), and returns a possibly transformed version of the document, still as an entire string. This can be used to remove HTML tags, lowercase the entire document, etc.\n",
    "\n",
    "\n",
    "- **tokenizer**: a callable that takes the output from the preprocessor and splits it into tokens, then returns a list of these.\n",
    "\n",
    "\n",
    "- **analyzer**: a callable that replaces the preprocessor and tokenizer. The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of how to encode sparse matrix fast\n",
    "\n",
    "\n",
    "Notice that in order to build our data as a matrix we need to use sparse matrices due to the high dimensionality (number of words/features) of the vocabulary.\n",
    "\n",
    "Here there is a little example to illustrate how we can build a csr_matrix (compressed sparse row matrix) fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2],\n",
       "       [2, 1, 0],\n",
       "       [0, 1, 3]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,0,2],[2,1,0],[0,1,3]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 2],\n",
       "        [2, 1, 0],\n",
       "        [0, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "data = [1,2,2,1,1,3]\n",
    "row = [0,0,1,1,2,2]\n",
    "col = [0,2,0,1,1,2]\n",
    "scipy.sparse.csr_matrix( (data,(row,col)), shape=(3,3) ).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Build a Simple countvectorizer\n",
    "\n",
    "Complete methods `fit` and `transform`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\\nSubject: Re: Shuttle Launch Question\\nOrganization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\\nDistribution: sci\\nLines: 23\\n\\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\\n>>>\"Clear caution & warning memory.  Verify no unexpected\\n>>>errors. ...\".  I am wondering what an \"expected error\" might\\n>>>be.  Sorry if this is a really dumb question, but\\n> \\n> Parity errors in memory or previously known conditions that were waivered.\\n>    \"Yes that is an error, but we already knew about it\"\\n> I\\'d be curious as to what the real meaning of the quote is.\\n> \\n> tom\\n\\n\\nMy understanding is that the \\'expected errors\\' are basically\\nknown bugs in the warning system software - things are checked\\nthat don\\'t have the right values in yet because they aren\\'t\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n\\'ok, if you see a warning no. 213 before liftoff, ignore it\\'.\\n\\n - Jonathan\\n\\n\\n',\n",
       " 14)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[4], y_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"David's car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-25-0ee3792f1c76>, line 97)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-0ee3792f1c76>\"\u001b[0;36m, line \u001b[0;32m97\u001b[0m\n\u001b[0;31m    self.word_to_ind =  None\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "class SimpleCountVectorizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_word_counts=1,\n",
    "                 doc_cleaner_pattern=r\"[^a-zA-Z]\",\n",
    "                 token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 dtype=np.float32,\n",
    "                 doc_cleaner_func=None,\n",
    "                 tokenizer_func=None,\n",
    "                 word_transformer_func=None):\n",
    "        \n",
    "        self._retype = type(re.compile('hello, world'))\n",
    "\n",
    "        self.min_word_counts     = min_word_counts\n",
    "        self.doc_cleaner_pattern = doc_cleaner_pattern\n",
    "        self.token_pattern       = token_pattern\n",
    "        self.dtype               = dtype\n",
    "        \n",
    "        self.doc_cleaner_func      = doc_cleaner_func\n",
    "        self.tokenizer_func        = tokenizer_func\n",
    "        self.word_transformer_func = word_transformer_func\n",
    "\n",
    "        self.vocabulary = set()\n",
    "        self.word_to_ind = {}\n",
    "\n",
    "\n",
    "    def build_doc_cleaner(self, lower=True):\n",
    "        \"\"\"\n",
    "        Returns a function that cleans undesirable substrings in a string.\n",
    "        It also lowers the input string if lower=True\n",
    "        \n",
    "        If `self.doc_cleaner_func` is specified when the method is instanciated\n",
    "        then that function is used to clean the tokens (defaults to None).\n",
    "        Otherwise `self.doc_cleaner_func` is constructed using a regular expression\n",
    "        (defaults to r\"[^a-zA-Z]\")\n",
    "        \"\"\"\n",
    "        if self.doc_cleaner_func:\n",
    "            return self.doc_cleaner_func\n",
    "        else:\n",
    "            clean_doc_pattern = re.compile(self.doc_cleaner_pattern)\n",
    "            if lower:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc).lower()\n",
    "            else:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc)\n",
    "\n",
    "    def build_tokenizer(self):\n",
    "        \"\"\"Returns a function that splits a string into a sequence of tokens\"\"\"\n",
    "        if self.tokenizer_func:\n",
    "            return self.tokenizer_func\n",
    "        \n",
    "        else:\n",
    "            token_pattern = re.compile(self.token_pattern)\n",
    "            return lambda doc: token_pattern.findall(doc)\n",
    "\n",
    "    def build_word_transformer(self):\n",
    "        \"\"\"Returns a stemmer or lemmaitzer if object has any\"\"\"\n",
    "        \n",
    "        if self.word_transformer_func:\n",
    "            return self.word_transformer_func\n",
    "        else:\n",
    "            return lambda word: word\n",
    "        \n",
    "    def tokenize(self, doc):\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        doc     = doc_cleaner(doc)\n",
    "        words = doc_tokenizer(doc)\n",
    "            \n",
    "        return words\n",
    "        \n",
    "    def fit(self, X):\n",
    "\n",
    "        assert self.vocabulary == set(), \"self.vocabulary is not empty it has {} words\".format(len(self.vocabulary))\n",
    "        assert isinstance(X,list), \"X is expected to be a list of documents\"\n",
    "        \n",
    "        i = 0\n",
    "        word_to_ind = {}\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()\n",
    "        \n",
    "        for x in X:\n",
    "            ### BEGIN EXERCISE ##########################################################\n",
    "            tokens = x.split(\" \")\n",
    "            for token in tokens:\n",
    "                #update word_to_ind\n",
    "\n",
    "\n",
    "        self.word_to_ind =  None     \n",
    "        self.n_features = None\n",
    "        ### END EXERCISE ##########################################################\n",
    "\n",
    "\n",
    "        self.vocabulary = set(word_to_ind.keys())\n",
    "        \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, memory_efficient=False):\n",
    "        \n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()      \n",
    "        \n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "        \n",
    "        if memory_efficient:\n",
    "            ### BEGIN EXERCISE ##########################################################\n",
    "            encoded_X = None # Create an encoded_X\n",
    "            \n",
    "            \n",
    "            for m, doc in enumerate(X):\n",
    "                # FILL IN THIS\n",
    "                words_from_doc = doc.split(\" \")\n",
    "                for word in words_from_doc:\n",
    "                    word_index = self.word_to_ind[word]\n",
    "                    encoded_X[m, word_index] += 1\n",
    "                \n",
    "            ### END EXERCISE  ##########################################################\n",
    "\n",
    "        ### You can try to do it if memory_efficient=False using np arrays\n",
    "        \n",
    "        return encoded_X\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        encoded_X = self.transform(X)\n",
    "        return encoded_X\n",
    "    \n",
    "    def _words_in_vocab(self, X):\n",
    "        \n",
    "        if isinstance(X, str):\n",
    "            return [w for w in self.tokenize(X) if w in self.vocabulary]\n",
    "        \n",
    "        X_words_in_vocab = []\n",
    "        for sentence in X:\n",
    "            X_words_in_vocab.append(self.tokenize(sentence))\n",
    "            \n",
    "        return X_words_in_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer = SimpleCountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer.word_transformer_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.25 s, sys: 65 ms, total: 4.31 s\n",
      "Wall time: 4.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleCountVectorizer(doc_cleaner_func=None, doc_cleaner_pattern='[^a-zA-Z]',\n",
       "           dtype=<class 'numpy.float32'>, min_word_counts=1,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer_func=None,\n",
       "           word_transformer_func=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "simple_count_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### [STRINGS] -> X matriu -> ML\n",
    "\n",
    "#### [STRINGS] -> X matriu -> ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14x89012 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_count_vectorizer.transform([[\"My house house\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.23 s, sys: 120 ms, total: 6.35 s\n",
      "Wall time: 6.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "X_train_matrix = simple_count_vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 89012), 11314)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_matrix.shape, len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.4 s, sys: 226 ms, total: 34.6 s\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "X_train_matrix2 = simple_count_vectorizer.transform(X_train, memory_efficient=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should a vector representation have the same number of items as words?\n",
    "\n",
    "There sum of the feature vector should equal to the number of words in a sentence if and only if all words belong to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCountVectorizer(doc_cleaner_func=None, doc_cleaner_pattern='[^a-zA-Z]',\n",
       "           dtype=<class 'numpy.float32'>, min_word_counts=1,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer_func=None,\n",
       "           word_transformer_func=None)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_count_vectorizer = SimpleCountVectorizer()\n",
    "simple_count_vectorizer.fit(X_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5991"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple_count_vectorizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5991 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [\"this is a feature vector for this sentence\"]\n",
    "x = simple_count_vectorizer.transform(A)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x5991 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [\"this is a feature vector for this sentence\",\"This is another sentence\"]\n",
    "x = simple_count_vectorizer.transform(A)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'feature', 'vector', 'for', 'this', 'sentence']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_count_vectorizer.tokenize(A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'feature', 'for', 'this']"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_vocab = [w for w in simple_count_vectorizer.tokenize(A[0]) if w in simple_count_vectorizer.vocabulary]\n",
    "words_in_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_in_vocab), len(set(words_in_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vector', 'sentence']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_notin_vocab = [w for w in simple_count_vectorizer.tokenize(A[0]) if w not in simple_count_vectorizer.vocabulary]\n",
    "words_notin_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training a document classifier with `SimpleCountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I) No Stemmer and no doc_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vainilla_count_vectorizer = SimpleCountVectorizer( doc_cleaner_func=lambda doc: doc)\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_0 = sklearn.pipeline.Pipeline([(\"countvectorizer\", vainilla_count_vectorizer),\n",
    "                                         (\"logisticregression\", logistic)],\n",
    "                                         )#memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9980555064521831    Accuracy test: 0.7999203398831651\n",
      "CPU times: user 1min 2s, sys: 689 ms, total: 1min 2s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_0.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_0.predict(X_test)\n",
    "y_train_pred = model_pipe_0.predict(X_train)\n",
    "\n",
    "acc_train_0 = np.mean(y_train == y_train_pred)\n",
    "acc_test_0 = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_0, acc_test_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x155448 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 94 stored elements in Compressed Sparse Row format>, 155448)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_0.steps[0][1].transform(X_train[0:1]), model_pipe_0.steps[0][1].n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(model_pipe_0.steps[0][1].vocabulary)\n",
    "vocabulary.sort()\n",
    "#vocabulary[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II) No stemmer but doc_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer = SimpleCountVectorizer(doc_cleaner_pattern=re.compile(\"[^a-zA-Z]\"))\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_1 = sklearn.pipeline.Pipeline([(\"countvectorizer\", simple_count_vectorizer),\n",
    "                                        (\"logisticregression\", logistic)],\n",
    "                                         )#memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.7 s, sys: 454 ms, total: 53.1 s\n",
      "Wall time: 53.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', SimpleCountVectorizer(doc_cleaner_func=None,\n",
       "           doc_cleaner_pattern=re.compile('[^a-zA-Z]'),\n",
       "           dtype=<class 'numpy.float32'>, min_word_counts=1,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer_func=None,\n",
       "           word_transformer_func=None)), ('l...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9973484178893406    Accuracy test: 0.80323951141795\n"
     ]
    }
   ],
   "source": [
    "y_test_pred  = model_pipe_1.predict(X_test)\n",
    "y_train_pred = model_pipe_1.predict(X_train)\n",
    "\n",
    "acc_train_1 = np.mean(y_train == y_train_pred)\n",
    "acc_test_1 = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_1, acc_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x89012 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 86 stored elements in Compressed Sparse Row format>, 89012)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_1.steps[0][1].transform(X_train[0:1]), model_pipe_1.steps[0][1].n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaa',\n",
       " 'aaaa',\n",
       " 'aaaaaaaaaaaa',\n",
       " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaauuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuugggggggggggggggg',\n",
       " 'aaaaagggghhhh',\n",
       " 'aaaarrgghhhh',\n",
       " 'aaah',\n",
       " 'aaahh',\n",
       " 'aaahhhh']"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(model_pipe_1.steps[0][1].vocabulary)\n",
    "vocabulary.sort()\n",
    "vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III) Use a SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer_stemmer = SimpleCountVectorizer(word_transformer_func= SnowballStemmer('english').stem,\n",
    "                                                        doc_cleaner_pattern=re.compile(\"[^a-zA-Z]\"))\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_2 = sklearn.pipeline.Pipeline([(\"countvectorizer\", simple_count_vectorizer_stemmer),\n",
    "                                        (\"logisticregression\", logistic)],\n",
    "                                         )#memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9966413293264982    Accuracy test: 0.8053637812002125\n",
      "CPU times: user 2min 10s, sys: 653 ms, total: 2min 11s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_2.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_2.predict(X_test)\n",
    "y_train_pred = model_pipe_2.predict(X_train)\n",
    "\n",
    "acc_train_2 = np.mean(y_train == y_train_pred)\n",
    "acc_test_2  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_2, acc_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x69965 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 84 stored elements in Compressed Sparse Row format>, 69965)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_2.steps[0][1].transform(X_train[0:1]), model_pipe_2.steps[0][1].n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaa',\n",
       " 'aaaa',\n",
       " 'aaaaaaaaaaaa',\n",
       " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaauuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuugggggggggggggggg',\n",
       " 'aaaaagggghhhh',\n",
       " 'aaaarrgghhhh',\n",
       " 'aaah',\n",
       " 'aaahh',\n",
       " 'aaahhhh']"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(model_pipe_1.steps[0][1].vocabulary)\n",
    "vocabulary.sort()\n",
    "vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table with results for each pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results[\"no clean no stem\"]   = [acc_train_0, acc_test_0]\n",
    "df_results[\"yes clean no stem\"]  = [acc_train_1, acc_test_1]\n",
    "df_results[\"yes clean yes stem\"] = [acc_train_2, acc_test_2]\n",
    "df_results.index=[\"train\",\"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.998056</td>\n",
       "      <td>0.997348</td>\n",
       "      <td>0.996641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.799920</td>\n",
       "      <td>0.803240</td>\n",
       "      <td>0.805364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem\n",
       "train          0.998056           0.997348            0.996641\n",
       "test           0.799920           0.803240            0.805364"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Ngram features with Sklearn vectorizer\n",
    "\n",
    "\n",
    "#### IV) Training a document classifier with sklearn `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_3 = sklearn.pipeline.Pipeline([(\"countvectorizer\", count_vectorizer),\n",
    "                                          (\"logisticregression\", logistic)],\n",
    "                                         )# memory='/Users/Shared/sklearn_mem/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9976135761004066    Accuracy test: 0.8070897503983006\n",
      "CPU times: user 29.6 s, sys: 239 ms, total: 29.9 s\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_3.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_3.predict(X_test)\n",
    "y_train_pred = model_pipe_3.predict(X_train)\n",
    "\n",
    "acc_train_3 = np.mean(y_train == y_train_pred)\n",
    "acc_test_3  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_3, acc_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_3.steps[0][1].transform(X_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results[\"sklearn countvectorizer\"] = [acc_train_3, acc_test_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "      <th>sklearn countvectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.998056</td>\n",
       "      <td>0.997348</td>\n",
       "      <td>0.996641</td>\n",
       "      <td>0.997614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.799920</td>\n",
       "      <td>0.803240</td>\n",
       "      <td>0.805364</td>\n",
       "      <td>0.807090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem  \\\n",
       "train          0.998056           0.997348            0.996641   \n",
       "test           0.799920           0.803240            0.805364   \n",
       "\n",
       "       sklearn countvectorizer  \n",
       "train                 0.997614  \n",
       "test                  0.807090  "
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V) Training a document classifier with sklearn `CountVectorizer` and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[The house was nice]\n",
    "\n",
    "0the \n",
    "1house\n",
    "2was \n",
    "3nice\n",
    "\n",
    "4the house\n",
    "5house was\n",
    "6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_4 = sklearn.pipeline.Pipeline([(\"countvectorizer\", count_vectorizer),\n",
    "                                        (\"logisticregression\", logistic)],\n",
    "                                        )# memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9994696835778681    Accuracy test: 0.8104089219330854\n",
      "CPU times: user 2min 2s, sys: 934 ms, total: 2min 3s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_4.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_4.predict(X_test)\n",
    "y_train_pred = model_pipe_4.predict(X_train)\n",
    "\n",
    "acc_train_4 = np.mean(y_train == y_train_pred)\n",
    "acc_test_4  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_4, acc_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1181803 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 202 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_4.steps[0][1].transform(X_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results[\"sklearn countvectorizer 2gram\"] = [acc_train_4, acc_test_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "      <th>sklearn countvectorizer</th>\n",
       "      <th>sklearn countvectorizer 2gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.998056</td>\n",
       "      <td>0.997348</td>\n",
       "      <td>0.996641</td>\n",
       "      <td>0.997614</td>\n",
       "      <td>0.999470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.799920</td>\n",
       "      <td>0.803240</td>\n",
       "      <td>0.805364</td>\n",
       "      <td>0.807090</td>\n",
       "      <td>0.810409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem  \\\n",
       "train          0.998056           0.997348            0.996641   \n",
       "test           0.799920           0.803240            0.805364   \n",
       "\n",
       "       sklearn countvectorizer  sklearn countvectorizer 2gram  \n",
       "train                 0.997614                       0.999470  \n",
       "test                  0.807090                       0.810409  "
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a35431550>"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a304dd5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.79,0.83))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SelectKbest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))\n",
    "feature_selector = SelectKBest(chi2, k = 700000)\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_5 = sklearn.pipeline.Pipeline([(\"count_vectorizer\", count_vectorizer),\n",
    "                                          (\"feature_selector\", feature_selector),\n",
    "                                          (\"logisticregression\", logistic)],\n",
    "                                         )# memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 516 ms, total: 1min 15s\n",
      "Wall time: 1min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       " ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_train = np.mean(model_pipe_5.predict(X_train) == y_train)\n",
    "acc_test = np.mean(model_pipe_5.predict(X_test) == y_test)\n",
    "df_results[\"sklearn countvectorizer 2gram + selection\"] = [acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a43e6af60>"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a43ed3da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.79,0.83))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3) Feature Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer_stemmer = SimpleCountVectorizer(word_transformer_func= SnowballStemmer('english').stem,\n",
    "                                                        doc_cleaner_pattern=re.compile(\"[^a-zA-Z]\"))\n",
    "\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "union = sklearn.pipeline.FeatureUnion([(\"simple_count_vectorizer_stemmer\", simple_count_vectorizer_stemmer),\n",
    "                                       (\"count_vectorizer\", count_vectorizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "feature_selector = SelectKBest(chi2, k = 700000)\n",
    "model_pipe_6 = sklearn.pipeline.Pipeline([(\"union_vectorizers\", union),\n",
    "                                          (\"feature_selector\", feature_selector),\n",
    "                                          (\"logisticregression\", logistic)],\n",
    "                                         )# memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/envs/py3/lib/python3.5/site-packages/sklearn/utils/__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 1s, sys: 4.58 s, total: 6min 5s\n",
      "Wall time: 6min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union_vectorizers', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('simple_count_vectorizer_stemmer', SimpleCountVectorizer(doc_cleaner_func=None,\n",
       "           doc_cleaner_pattern=re.compile('[^a-zA-Z]'),\n",
       "           dtype=<class 'numpy.float32'>, min_word_counts=1,\n",
       "           token_pattern=...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/envs/py3/lib/python3.5/site-packages/sklearn/utils/__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-af2bf7dbd2e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pipe_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pipe_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Feature union + selection\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_results' is not defined"
     ]
    }
   ],
   "source": [
    "acc_train = np.mean(model_pipe_6.predict(X_train) == y_train)\n",
    "acc_test = np.mean(model_pipe_6.predict(X_test) == y_test)\n",
    "df_results[\"Feature union + selection\"] = [acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.79,0.83))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Crossvalidating results (Exercise)\n",
    "\n",
    "It is very (VERY) important you do crossvalidation. \n",
    "\n",
    "\n",
    "If you have never use Bayesian Optimization for selecting hyperparameters I recommend you to try: https://scikit-optimize.github.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can't deepcopy objects with compiled regular expressions in python 3.6\n",
    "# Therefore can't use GridSearch with our doc_cleaner with a compiled regular expression\n",
    "# In python 3.7 you can!!\n",
    "# Minimal working example\n",
    "#import re,copy\n",
    "#class MyClass():\n",
    "#    def __init__(self):\n",
    "#        self.regex=re.compile('\\d+')\n",
    "#\n",
    "#myobj = MyClass()    \n",
    "#copy.deepcopy(myobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train_vec = union.fit_transform(X_train)\n",
    "#n_features = X_train_vec.shape[1]\n",
    "#min_k = n_features//2\n",
    "#max_k = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer_stemmer = SimpleCountVectorizer(lemmatizer= None,\n",
    "                                                        stemmer= SnowballStemmer('english'),\n",
    "                                                        doc_cleaner=r\"[^a-zA-Z]\")\n",
    "\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "union = sklearn.pipeline.FeatureUnion([(\"simple_count_vectorizer_stemmer\", simple_count_vectorizer_stemmer),\n",
    "                                       (\"count_vectorizer\", count_vectorizer)])\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "feature_selector = SelectKBest(chi2)\n",
    "\n",
    "\n",
    "# Hint\n",
    "#model_pipe_6 = sklearn.pipeline.Pipeline([(\"union_vectorizers\", union),\n",
    "#                                          (\"feature_selector\", feature_selector),\n",
    "#                                          (\"logisticregression\", logistic)],\n",
    "#                                         )# memory='/Users/Shared/sklearn_mem/')\n",
    "\n",
    "#possible_K = [int(x)-1 for x in np.linspace(min_k, max_k,10)]\n",
    "#parameteres = {'feature_selector__k':possible_K, 'logisticregression__C':[0.1,0.01,0.001]}\n",
    "\n",
    "#grid_6 = sklearn.model_selection.RandomizedSearchCV(model_pipe_6,\n",
    "#                                                    param_distributions=parameteres, \n",
    "#                                                    cv=3,\n",
    "#                                                    n_iter=2,\n",
    "#                                                    n_jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%time\n",
    "#grid_6.fit(X_train, y_train)\n",
    "# Returns error? Welcome to your custom sklearn pipelines\n",
    "# Question: Why is there an error?\n",
    "# ValueError: k should be >=0, <= n_features; got 3238254.Use k='all' to return all features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#acc_train = np.mean(grid_6.predict(X_train) == y_train)\n",
    "#acc_test = np.mean(grid_6.predict(X_test) == y_test)\n",
    "#print(\"acc_train={} acc_test={}\".format(acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_results[\"Feature union + selection + CV\"] = [acc_train, acc_test]\n",
    "#df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Hashing words (Exercise)\n",
    "\n",
    "Explore the results you get by hashing the input words. Add `df_results[\"Feature hash + selection + CV\"]` in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
